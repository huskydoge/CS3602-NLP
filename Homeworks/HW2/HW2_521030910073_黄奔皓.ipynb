{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作业二：实现Word2Vec的连续词袋模型\n",
    "姓名： 黄奔皓\n",
    "学号： 521030910073"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple, http://pypi.douban.com/simple, https://pypi.tuna.tsinghua.edu.cn/simple, http://pypi.mirrors.ustc.edu.cn/simple, http://pypi.hustunique.com\r\n",
      "Requirement already satisfied: numpy in /Users/husky/opt/anaconda3/lib/python3.9/site-packages (1.22.4)\r\n",
      "Requirement already satisfied: tqdm in /Users/husky/opt/anaconda3/lib/python3.9/site-packages (4.65.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T05:51:53.269040Z",
     "start_time": "2023-11-13T05:51:40.856227Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要Python版本大于等于3.6，并检查是否已安装所依赖的第三方库。（若没有安装可以执行上面的代码块）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-13T05:56:09.166298Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 390\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/gv/jp8cgv_s7sdcy5__5j0p3l7r0000gn/T/ipykernel_76712/2556787213.py\", line 12, in <module>\n",
      "    importlib.import_module(name)\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 982, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 925, in _find_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1423, in find_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1392, in _get_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1356, in _path_importer_cache\n",
      "PermissionError: [Errno 1] Operation not permitted\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'PermissionError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/inspect.py\", line 1543, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/inspect.py\", line 1501, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/inspect.py\", line 709, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/inspect.py\", line 738, in getmodule\n",
      "    file = getabsfile(object, _filename)\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/inspect.py\", line 722, in getabsfile\n",
      "    return os.path.normcase(os.path.abspath(_filename))\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/posixpath.py\", line 380, in abspath\n",
      "    cwd = os.getcwd()\n",
      "PermissionError: [Errno 1] Operation not permitted\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/gv/jp8cgv_s7sdcy5__5j0p3l7r0000gn/T/ipykernel_76712/2556787213.py\", line 12, in <module>\n",
      "    importlib.import_module(name)\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 982, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 925, in _find_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1423, in find_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1392, in _get_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1356, in _path_importer_cache\n",
      "PermissionError: [Errno 1] Operation not permitted\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'PermissionError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3364, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3461, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2066, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(etype,\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/inspect.py\", line 1543, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/inspect.py\", line 1501, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/inspect.py\", line 709, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/inspect.py\", line 738, in getmodule\n",
      "    file = getabsfile(object, _filename)\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/inspect.py\", line 722, in getabsfile\n",
      "    return os.path.normcase(os.path.abspath(_filename))\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/posixpath.py\", line 380, in abspath\n",
      "    cwd = os.getcwd()\n",
      "PermissionError: [Errno 1] Operation not permitted\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/gv/jp8cgv_s7sdcy5__5j0p3l7r0000gn/T/ipykernel_76712/2556787213.py\", line 12, in <module>\n",
      "    importlib.import_module(name)\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 982, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 925, in _find_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1423, in find_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1392, in _get_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1356, in _path_importer_cache\n",
      "PermissionError: [Errno 1] Operation not permitted\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'PermissionError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3364, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3461, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2066, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(etype,\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2947, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3172, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3383, in run_ast_nodes\n",
      "    self.showtraceback()\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2066, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(etype,\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1142, in structured_traceback\n",
      "    formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/inspect.py\", line 1543, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/inspect.py\", line 1501, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/inspect.py\", line 709, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/inspect.py\", line 738, in getmodule\n",
      "    file = getabsfile(object, _filename)\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/inspect.py\", line 722, in getabsfile\n",
      "    return os.path.normcase(os.path.abspath(_filename))\n",
      "  File \"/Users/husky/opt/anaconda3/lib/python3.9/posixpath.py\", line 380, in abspath\n",
      "    cwd = os.getcwd()\n",
      "PermissionError: [Errno 1] Operation not permitted\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import sys\n",
    "\n",
    "assert sys.version_info[0] == 3\n",
    "assert sys.version_info[1] >= 6\n",
    "\n",
    "requirements = [\"numpy\", \"tqdm\"]\n",
    "_OK = True\n",
    "\n",
    "for name in requirements:\n",
    "    try:\n",
    "        importlib.import_module(name)\n",
    "    except ImportError:\n",
    "        print(f\"Require: {name}\")\n",
    "        _OK = False\n",
    "\n",
    "if not _OK:\n",
    "    exit(-1)\n",
    "else:\n",
    "    print(\"All libraries are satisfied.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 辅助代码\n",
    "\n",
    "该部分包含：用于给句子分词的分词器`tokenizer`、用于构造数据的数据集类`Dataset`和用于构建词表的词表类`Vocab`。\n",
    "\n",
    "> 注: 该部分无需实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分词器\n",
    "\n",
    "该分词器会：\n",
    "1. 将所有字母转为小写；\n",
    "2. 将句子分为连续的字母序列（word）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T05:56:42.945111Z",
     "start_time": "2023-11-13T05:56:42.941486Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 's', 'useful']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def tokenizer(line: str) -> List[str]:\n",
    "    line = line.lower()\n",
    "    tokens = list(filter(lambda x: len(x) > 0, re.split(r\"\\W\", line)))\n",
    "    return tokens\n",
    "\n",
    "\n",
    "print(tokenizer(\"It's  useful. \"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集类\n",
    "\n",
    "通过设定窗长`window_size`（上下文窗口），该数据集类会读取`corpus`中的行，并解析返回`(context, target)`元组。\n",
    "\n",
    "假如一个句子序列为`a b c d e`，且此时`window_size=2`，`Dataset`会返回：\n",
    "\n",
    "```\n",
    "([b, c], a)\n",
    "([a, c, d], b)\n",
    "([a, b, d, e], c)\n",
    "([b, c, e], d)\n",
    "([c, d], e)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T06:29:59.631035Z",
     "start_time": "2023-11-13T06:29:59.629320Z"
    }
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, corpus: str, window_size: int):\n",
    "        \"\"\"\n",
    "        :param corpus: 语料路径\n",
    "        :param window_size: 窗口长度\n",
    "        \"\"\"\n",
    "        self.corpus = corpus\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        with open(self.corpus, encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                tokens = tokenizer(line)\n",
    "                if len(tokens) <= 1:\n",
    "                    continue\n",
    "                for i, target in enumerate(tokens):\n",
    "                    left_context = tokens[max(0, i - self.window_size): i]\n",
    "                    right_context = tokens[i + 1: i + 1 + self.window_size]\n",
    "                    context = left_context + right_context\n",
    "                    yield context, target\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" 统计样本语料中的样本个数 \"\"\"\n",
    "        len_ = getattr(self, \"len_\", None)\n",
    "        if len_ is not None:\n",
    "            return len_\n",
    "\n",
    "        len_ = 0\n",
    "        for _ in iter(self):\n",
    "            len_ += 1\n",
    "\n",
    "        setattr(self, \"len_\", len_)\n",
    "        return len_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T06:30:05.287369Z",
     "start_time": "2023-11-13T06:30:05.282447Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "(['want', 'to', 'go'], 'i')\n",
      "(['i', 'to', 'go', 'home'], 'want')\n",
      "(['i', 'want', 'go', 'home'], 'to')\n",
      "(['i', 'want', 'to', 'home'], 'go')\n"
     ]
    }
   ],
   "source": [
    "debug_dataset = Dataset(\"./data/debug.txt\", window_size=3)\n",
    "print(len(debug_dataset))\n",
    "\n",
    "for i, pair in enumerate(iter(debug_dataset)):\n",
    "    print(pair)\n",
    "    if i >= 3:\n",
    "        break\n",
    "\n",
    "del debug_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词表类\n",
    "\n",
    "`Vocab`可以用`token_to_idx`把token(str)映射为索引(int)，也可以用`idx_to_token`找到索引对应的token。\n",
    "\n",
    "实例化`Vocab`有两种方法：\n",
    "1. 读取`corpus`构建词表。\n",
    "2. 通过调用`Vocab.load_vocab`，可以从已训练的中构建`Vocab`实例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T06:31:59.053277Z",
     "start_time": "2023-11-13T06:31:59.050269Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "\n",
    "class Vocab:\n",
    "    VOCAB_FILE = \"vocab.txt\"\n",
    "    UNK = \"<unk>\"\n",
    "\n",
    "    def __init__(self, corpus: str = None, max_vocab_size: int = -1):\n",
    "        \"\"\"\n",
    "        :param corpus:         语料文件路径\n",
    "        :param max_vocab_size: 最大词表数量，-1表示不做任何限制\n",
    "        \"\"\"\n",
    "        self._token_to_idx: Dict[str, int] = {}\n",
    "        self.token_freq: List[Tuple[str, int]] = []\n",
    "\n",
    "        if corpus is not None:\n",
    "            self.build_vocab(corpus, max_vocab_size)\n",
    "\n",
    "    def build_vocab(self, corpus: str, max_vocab_size: int = -1):\n",
    "        \"\"\" 统计词频，并保留高频词 \"\"\"\n",
    "        counter = Counter()\n",
    "        with open(corpus, encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                tokens = tokenizer(line)\n",
    "                counter.update(tokens)\n",
    "\n",
    "        print(f\"总Token数: {sum(counter.values())}\")\n",
    "\n",
    "        # 将找到的词按照词频从高到低排序\n",
    "        self.token_freq = [(self.UNK, 1)] + sorted(counter.items(),\n",
    "                                                   key=lambda x: x[1], reverse=True)\n",
    "        if max_vocab_size > 0:\n",
    "            self.token_freq = self.token_freq[:max_vocab_size]\n",
    "\n",
    "        print(f\"词表大小: {len(self.token_freq)}\")\n",
    "\n",
    "        for i, (token, _freq) in enumerate(self.token_freq):\n",
    "            self._token_to_idx[token] = i\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_freq)\n",
    "\n",
    "    def __contains__(self, token: str):\n",
    "        return token in self._token_to_idx\n",
    "\n",
    "    def token_to_idx(self, token: str, warn: bool = False) -> int:\n",
    "        \"\"\" Map the token to index \"\"\"\n",
    "        token = token.lower()\n",
    "        if token not in self._token_to_idx:\n",
    "            if warn:\n",
    "                warnings.warn(f\"{token} => {self.UNK}\")\n",
    "            token = self.UNK\n",
    "        return self._token_to_idx[token]\n",
    "\n",
    "    def idx_to_token(self, idx: int) -> str:\n",
    "        \"\"\" Map the index to token \"\"\"\n",
    "        assert 0 <= idx < len(self)\n",
    "        return self.token_freq[idx][0]\n",
    "\n",
    "    def save_vocab(self, path: str):\n",
    "        with open(os.path.join(path, self.VOCAB_FILE), \"w\", encoding=\"utf8\") as f:\n",
    "            lines = [f\"{token} {freq}\" for token, freq in self.token_freq]\n",
    "            f.write(\"\\n\".join(lines))\n",
    "\n",
    "    @classmethod\n",
    "    def load_vocab(cls, path: str):\n",
    "        vocab = cls()\n",
    "\n",
    "        with open(os.path.join(path, cls.VOCAB_FILE), encoding=\"utf8\") as f:\n",
    "            lines = f.read().split(\"\\n\")\n",
    "\n",
    "        for i, line in enumerate(lines):\n",
    "            token, freq = line.split()\n",
    "            vocab.token_freq.append((token, int(freq)))\n",
    "            vocab._token_to_idx[token] = i\n",
    "\n",
    "        return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T06:32:02.713179Z",
     "start_time": "2023-11-13T06:32:02.706280Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总Token数: 50\n",
      "词表大小: 21\n",
      "[('<unk>', 1), ('want', 6), ('to', 6), ('go', 4), ('i', 3), ('home', 3), ('play', 3), ('like', 3), ('eating', 3), ('he', 3), ('she', 3), ('it', 2), ('is', 2), ('we', 2), ('useful', 1), ('awful', 1), ('can', 1), ('read', 1), ('books', 1), ('will', 1), ('now', 1)]\n"
     ]
    }
   ],
   "source": [
    "debug_vocab = Vocab(\"./data/debug.txt\")\n",
    "print(debug_vocab.token_freq)\n",
    "del debug_vocab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec实现\n",
    "\n",
    "本节将实现Word2Vec的CBOW模型，为了便于实现，本实验不引入`Hierarchical Softmax`和` Negative Sampling`等加速技巧，若同学们对这些技术感兴趣，可参考：[word2vec Parameter Learning Explained](https://arxiv.org/pdf/1411.2738.pdf)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 实现one-hot向量构建函数(1分)\n",
    "\n",
    "需求：指定词向量的维度和需要置1的索引，返回类型为`np.ndarray`的one-hot行向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T06:53:38.882512Z",
     "start_time": "2023-11-13T06:53:38.704787Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def one_hot(dim: int, idx: int) -> np.ndarray:\n",
    "    # TODO: 实现one-hot函数（1分）\n",
    "    x = np.zeros(dim)\n",
    "    x[idx] = 1\n",
    "    return x\n",
    "\n",
    "\n",
    "print(one_hot(4, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO：实现softmax(2分)\n",
    "\n",
    "> 注意数值溢出的可能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T06:57:13.787851Z",
     "start_time": "2023-11-13T06:57:13.780543Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.80134161e-05 2.12062451e-04 5.76445508e-04 1.56694135e-03\n",
      " 4.25938820e-03 1.15782175e-02 3.14728583e-02 8.55520989e-02\n",
      " 2.32554716e-01 6.32149258e-01]\n"
     ]
    }
   ],
   "source": [
    "def softmax(x: np.ndarray) -> np.ndarray:\n",
    "    # TODO: 实现softmax函数（2分）\n",
    "    # 注意数值溢出\n",
    "    max = np.max(x)\n",
    "    x = [i - max for i in x] # 防止溢出\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "\n",
    "print(softmax(np.array([i for i in range(10)])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO：CBOW类，请补全`train_one_step`中的代码。\n",
    "\n",
    "推荐按照TODO描述的步骤来实现（预计15行代码），也可在保证结果正确的前提下按照自己的思路来实现。\n",
    "\n",
    "> tips: 建议使用numpy的向量化操作代替Python循环。\n",
    "> 比如同样是实现两个向量`a`和`b`的内积，`np.dot(a,b)`的运行效率可达纯Python实现的函数的百倍以上。同样的，向量外积也推荐使用`np.outer(a,b)`。具体的函数功能可参考Numpy文档。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T07:37:17.769900Z",
     "start_time": "2023-11-13T07:37:17.767304Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class CBOW:\n",
    "    def __init__(self, vocab: Vocab, vector_dim: int):\n",
    "        self.vocab = vocab\n",
    "        self.vector_dim = vector_dim\n",
    "\n",
    "        self.U = np.random.uniform(-1, 1, (len(self.vocab), self.vector_dim))  # vocab_size x vector_dim\n",
    "        self.V = np.random.uniform(-1, 1, (self.vector_dim, len(self.vocab)))  # vector_dim x vocab_size\n",
    "\n",
    "    def train(self, corpus: str, window_size: int, train_epoch: int, learning_rate: float, save_path: str = None):\n",
    "        dataset = Dataset(corpus, window_size)\n",
    "        start_time = time.time()\n",
    "\n",
    "        for epoch in range(1, train_epoch + 1):\n",
    "            self.train_one_epoch(epoch, dataset, learning_rate)\n",
    "            if save_path is not None:\n",
    "                self.save_model(save_path)\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(f\"总耗时 {end_time - start_time:.2f}s\")\n",
    "\n",
    "    def train_one_epoch(self, epoch: int, dataset: Dataset, learning_rate: float):\n",
    "        steps, total_loss = 0, 0.0\n",
    "\n",
    "        with tqdm(iter(dataset), total=len(dataset), desc=f\"Epoch {epoch}\", ncols=80) as pbar:\n",
    "            for sample in pbar:\n",
    "                context_tokens, target_token = sample\n",
    "                loss = self.train_one_step(context_tokens, target_token, learning_rate)\n",
    "                total_loss += loss\n",
    "                steps += 1\n",
    "                if steps % 10 == 0:\n",
    "                    pbar.set_postfix({\"Avg. loss\": f\"{total_loss / steps:.2f}\"})\n",
    "\n",
    "        return total_loss / steps\n",
    "\n",
    "    def train_one_step(self, context_tokens: List[str], target_token: str, learning_rate: float) -> float:\n",
    "        \"\"\"\n",
    "        :param context_tokens:  目标词周围的词\n",
    "        :param target_token:    目标词\n",
    "        :param learning_rate:   学习率\n",
    "        :return:    loss值 (标量)\n",
    "        \"\"\"\n",
    "        C = len(context_tokens)\n",
    "\n",
    "        # TODO: 构造输入向量和目标向量（3分）\n",
    "        # context: 构造输入向量\n",
    "        # target:  目标one-hot向量\n",
    "        context_token_idx = np.array([one_hot(len(self.vocab), self.vocab.token_to_idx(token)) for token in context_tokens])\n",
    "        target = np.array(one_hot(len(self.vocab), self.vocab.token_to_idx(target_token))) # 1 * vocab_size\n",
    "\n",
    "        # TODO: 前向步骤（3分）\n",
    "        # X: 词向量加和\n",
    "        # h: 隐层向量\n",
    "        # y: 预测向量\n",
    "        X = np.sum(context_token_idx, axis=0)\n",
    "        h = (X.T @ self.U) / C # use average,  1 * vector_dim\n",
    "        y = softmax(h @ self.V) # 1 * vocab_size\n",
    "        \n",
    "        # TODO: 计算loss（3分）\n",
    "        loss = -np.log(y @ target)\n",
    "\n",
    "        # TODO: 更新参数（3分）\n",
    "        # grad_V: V的梯度\n",
    "        # grad_U: U的梯度\n",
    "        e = y - target # 1 * vocab_size\n",
    "        grad_V = np.outer(h, e) # vector_dim * vocab_size\n",
    "        grad_U = np.outer(X, e @ self.V.T) / C # vocab_size * vector_dim\n",
    "        \n",
    "        self.V = self.V - learning_rate * grad_V\n",
    "        self.U = self.U - learning_rate * grad_U\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def similarity(self, token1: str, token2: str):\n",
    "        \"\"\" 计算两个词的相似性 \"\"\"\n",
    "        v1 = self.U[self.vocab.token_to_idx(token1)]\n",
    "        v2 = self.U[self.vocab.token_to_idx(token2)]\n",
    "        v1 = v1 / np.linalg.norm(v1)\n",
    "        v2 = v2 / np.linalg.norm(v2)\n",
    "        return np.dot(v1, v2)\n",
    "\n",
    "    def most_similar_tokens(self, token: str, n: int):\n",
    "        \"\"\" 召回与token最相似的n个token \"\"\"\n",
    "        norm_U = self.U / np.linalg.norm(self.U, axis=1, keepdims=True)\n",
    "\n",
    "        idx = self.vocab.token_to_idx(token, warn=True)\n",
    "        v = norm_U[idx]\n",
    "\n",
    "        cosine_similarity = np.dot(norm_U, v)\n",
    "        nbest_idx = np.argsort(cosine_similarity)[-n:][::-1]\n",
    "\n",
    "        results = []\n",
    "        for idx in nbest_idx:\n",
    "            _token = self.vocab.idx_to_token(idx)\n",
    "            results.append((_token, cosine_similarity[idx]))\n",
    "\n",
    "        return results\n",
    "\n",
    "    def save_model(self, path: str):\n",
    "        \"\"\" 将模型保存到`path`路径下，如果不存在`path`会主动创建 \"\"\"\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        self.vocab.save_vocab(path)\n",
    "\n",
    "        with open(os.path.join(path, \"wv.pkl\"), \"wb\") as f:\n",
    "            param = {\"U\": self.U, \"V\": self.V}\n",
    "            pickle.dump(param, f)\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(cls, path: str):\n",
    "        \"\"\" 从`path`加载模型 \"\"\"\n",
    "        vocab = Vocab.load_vocab(path)\n",
    "\n",
    "        with open(os.path.join(path, \"wv.pkl\"), \"rb\") as f:\n",
    "            param = pickle.load(f)\n",
    "\n",
    "        U, V = param[\"U\"], param[\"V\"]\n",
    "        model = cls(vocab, U.shape[1])\n",
    "        model.U, model.V = U, V\n",
    "\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试\n",
    "\n",
    "测试部分可用于验证CBOW实现的正确性，此部分的结果不计入总分。\n",
    "\n",
    "### 测试1\n",
    "\n",
    "本测试可用于调试，最终一个epoch的平均loss约为0.5，并且“i”、“he”和“she”的相似性较高。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T07:37:20.131463Z",
     "start_time": "2023-11-13T07:37:20.039798Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总Token数: 50\n",
      "词表大小: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████| 50/50 [00:00<00:00, 6930.90it/s, Avg. loss=2.89]\n",
      "Epoch 2: 100%|████████████████| 50/50 [00:00<00:00, 8260.73it/s, Avg. loss=1.54]\n",
      "Epoch 3: 100%|████████████████| 50/50 [00:00<00:00, 8609.00it/s, Avg. loss=1.05]\n",
      "Epoch 4: 100%|████████████████| 50/50 [00:00<00:00, 8941.55it/s, Avg. loss=0.82]\n",
      "Epoch 5: 100%|████████████████| 50/50 [00:00<00:00, 9438.98it/s, Avg. loss=0.76]\n",
      "Epoch 6: 100%|████████████████| 50/50 [00:00<00:00, 8481.57it/s, Avg. loss=0.67]\n",
      "Epoch 7: 100%|████████████████| 50/50 [00:00<00:00, 8801.58it/s, Avg. loss=0.53]\n",
      "Epoch 8: 100%|████████████████| 50/50 [00:00<00:00, 8675.96it/s, Avg. loss=0.54]\n",
      "Epoch 9: 100%|████████████████| 50/50 [00:00<00:00, 7410.69it/s, Avg. loss=0.52]\n",
      "Epoch 10: 100%|███████████████| 50/50 [00:00<00:00, 7263.37it/s, Avg. loss=0.50]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总耗时 0.07s\n",
      "[('i', 1.0), ('he', 0.9925540605382075), ('she', 0.9663378567626819), ('<unk>', 0.635699270231461), ('is', 0.39741235376377426)]\n",
      "[('he', 1.0), ('i', 0.9925540605382075), ('she', 0.98580084006031), ('<unk>', 0.617101792529352), ('is', 0.3582327872195812)]\n",
      "[('she', 1.0), ('he', 0.98580084006031), ('i', 0.9663378567626819), ('<unk>', 0.5012279262065746), ('is', 0.38698246680231224)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def test1():\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    vocab = Vocab(corpus=\"./data/debug.txt\")\n",
    "    cbow = CBOW(vocab, vector_dim=8)\n",
    "    cbow.train(corpus=\"./data/debug.txt\", window_size=3,\n",
    "               train_epoch=10, learning_rate=1.0)\n",
    "\n",
    "    print(cbow.most_similar_tokens(\"i\", 5))\n",
    "    print(cbow.most_similar_tokens(\"he\", 5))\n",
    "    print(cbow.most_similar_tokens(\"she\", 5))\n",
    "\n",
    "\n",
    "test1()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试2\n",
    "\n",
    "本测试将会在`treebank.txt`上训练词向量，为了加快训练流程，实验只保留高频的4000词，且词向量维度为20。\n",
    "\n",
    "在每个epoch结束后，会在`data/treebank.txt`中测试词向量的召回能力。如下所示，`data/treebank.txt`中每个样例为`word`以及对应的同义词，同义词从wordnet中获取。\n",
    "\n",
    "```\n",
    "[\n",
    "  \"about\",\n",
    "  [\n",
    "    \"most\",\n",
    "    \"virtually\",\n",
    "    \"around\",\n",
    "    \"almost\",\n",
    "    \"near\",\n",
    "    \"nearly\",\n",
    "    \"some\"\n",
    "  ]\n",
    "]\n",
    "```\n",
    "\n",
    "> 本阶段预计消耗25分钟，具体时间与`train_one_step`代码实现有关\n",
    "\n",
    "> 最后一个epoch平均loss降至5.1左右，并且在同义词上的召回率约为20%左右"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T08:34:55.580025Z",
     "start_time": "2023-11-13T07:37:32.368873Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总Token数: 205068\n",
      "词表大小: 4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|█████████| 205058/205058 [14:43<00:00, 232.08it/s, Avg. loss=5.99]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall rate: 8.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|█████████| 205058/205058 [04:28<00:00, 764.48it/s, Avg. loss=5.59]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall rate: 13.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|█████████| 205058/205058 [04:50<00:00, 704.71it/s, Avg. loss=5.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall rate: 14.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|█████████| 205058/205058 [05:07<00:00, 665.83it/s, Avg. loss=5.34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall rate: 15.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|█████████| 205058/205058 [04:31<00:00, 754.74it/s, Avg. loss=5.26]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall rate: 17.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|█████████| 205058/205058 [04:44<00:00, 720.22it/s, Avg. loss=5.20]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall rate: 18.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|█████████| 205058/205058 [04:50<00:00, 705.15it/s, Avg. loss=5.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall rate: 19.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|█████████| 205058/205058 [04:39<00:00, 733.26it/s, Avg. loss=5.11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall rate: 19.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|█████████| 205058/205058 [04:37<00:00, 739.59it/s, Avg. loss=5.07]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall rate: 20.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|████████| 205058/205058 [04:47<00:00, 713.23it/s, Avg. loss=5.04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall rate: 19.53%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def calculate_recall_rate(model: CBOW, word_synonyms: List[Tuple[str, List[str]]], topn: int) -> float:\n",
    "    \"\"\" 测试CBOW的召回率 \"\"\"\n",
    "    hit, total = 0, 1e-9\n",
    "    for word, synonyms in word_synonyms:\n",
    "        synonyms = set(synonyms)\n",
    "        recalled = set([w for w, _ in model.most_similar_tokens(word, topn)])\n",
    "        hit += len(synonyms & recalled)\n",
    "        total += len(synonyms)\n",
    "\n",
    "    print(f\"Recall rate: {hit / total:.2%}\")\n",
    "    return hit / total\n",
    "\n",
    "\n",
    "def test2():\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    corpus = \"./data/treebank.txt\"\n",
    "    lr = 1e-1\n",
    "    topn = 40\n",
    "\n",
    "    vocab = Vocab(corpus, max_vocab_size=4000)\n",
    "    model = CBOW(vocab, vector_dim=20)\n",
    "\n",
    "    dataset = Dataset(corpus, window_size=4)\n",
    "\n",
    "    with open(\"data/synonyms.json\", encoding=\"utf8\") as f:\n",
    "        word_synonyms: List[Tuple[str, List[str]]] = json.load(f)\n",
    "\n",
    "    for epoch in range(1, 11):\n",
    "        model.train_one_epoch(epoch, dataset, learning_rate=lr)\n",
    "        calculate_recall_rate(model, word_synonyms, topn)\n",
    "\n",
    "\n",
    "test2()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 实验总结\n",
    "\n",
    "本次实验主要是实现了CBOW模型，通过对语料的训练，得到了词向量，通过词向量的相似性，可以得到同义词，这样就可以对语料进行分析，得到语料的一些特征，比如语义相似性，语义距离等等。通过这次实验，我对词向量有了更深的理解，也对词向量的应用有了更深的认识。\n",
    "\n",
    "在实验过程中，我体会到CBOW在运行时间上确实仍需要改进，我约莫花费了一个小时的时间才完成test2的实验，可见 Hierarchical Softmax 和 Negative Sampling方法的重要性。\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 ('main')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "f18f9dacbff0d5957274e7eab45e9a081b8e58c43f44c2ad827db9dd3cd49b6f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
