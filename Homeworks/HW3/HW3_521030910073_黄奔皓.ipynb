{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d3413968a8ac85"
      },
      "source": [
        "# 作业三：预训练语言模型计算PPL\n",
        "姓名：黄奔皓\n",
        "学号：521030910073"
      ],
      "id": "2d3413968a8ac85"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-11T05:18:26.602125Z",
          "start_time": "2023-12-11T05:18:25.575859Z"
        },
        "id": "caf8f3bcf692fbca"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2TokenizerFast"
      ],
      "id": "caf8f3bcf692fbca"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-11T05:14:23.929710Z",
          "start_time": "2023-12-11T05:14:23.925565Z"
        },
        "id": "77b99d1c9af4521c"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "id": "77b99d1c9af4521c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7ccccf770ec5a22"
      },
      "source": [
        "## 加载模型和Tokenizer"
      ],
      "id": "b7ccccf770ec5a22"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-11T05:14:25.483065Z",
          "start_time": "2023-12-11T05:14:25.434606Z"
        },
        "id": "9cd1cf01cea25061"
      },
      "outputs": [],
      "source": [
        "model_path = \"gpt2\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_path).to(device)\n",
        "model.eval()\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(model_path)"
      ],
      "id": "9cd1cf01cea25061"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5500f44d51c7a12"
      },
      "source": [
        "### Tokenizer\n",
        "\n",
        "> 下面是一个例子，展示Tokenizer和模型的使用。理解下面的例子可能对你的大作业有帮助。\n",
        "\n",
        "```python\n",
        "\n",
        "Tokenizer会将句子分割成一个个token，然后将每个token转化为一个数字，这个数字就是这个token在词表中的id。"
      ],
      "id": "b5500f44d51c7a12"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f24d0191beab645c",
        "outputId": "1d564225-6c31-4622-acc7-339ca3074b28"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[   38, 11571,    12,    17,   318,   257,  6121,   364,  2746,  2181,\n",
              "         13363,   319,   257,   845,  1588, 35789,   286,  3594,  1366,   287,\n",
              "           257,  2116,    12, 16668, 16149,  6977,    13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "inputs = tokenizer(\"\"\"GPT-2 is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion.\"\"\", return_tensors=\"pt\")\n",
        "inputs"
      ],
      "id": "f24d0191beab645c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3de748feb4debd73"
      },
      "source": [
        "可以将token id映射到对应的分词token"
      ],
      "id": "3de748feb4debd73"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85f722f704f61ad6",
        "outputId": "7cc3a2b1-431d-4687-ecb9-e7b607f9da93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['G', 'PT', '-', '2', 'Ġis', 'Ġa', 'Ġtransform', 'ers', 'Ġmodel', 'Ġpret', 'rained', 'Ġon', 'Ġa', 'Ġvery', 'Ġlarge', 'Ġcorpus', 'Ġof', 'ĠEnglish', 'Ġdata', 'Ġin', 'Ġa', 'Ġself', '-', 'super', 'vised', 'Ġfashion', '.']\n"
          ]
        }
      ],
      "source": [
        "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "print(tokens)"
      ],
      "id": "85f722f704f61ad6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fda737fe0ec1de27"
      },
      "source": [
        "可以使用`decode`方法将token id转化回原来的句子"
      ],
      "id": "fda737fe0ec1de27"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "440261abfdf6821",
        "outputId": "5ce88864-6bc9-4697-e9ec-4f6a361b4eeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT-2 is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion.\n"
          ]
        }
      ],
      "source": [
        "decoded_string = tokenizer.decode([38, 11571, 12, 17, 318, 257, 6121, 364, 2746, 2181, 13363, 319, 257, 845, 1588, 35789, 286, 3594, 1366, 287, 257, 2116, 12, 16668, 16149, 6977, 13])\n",
        "print(decoded_string)"
      ],
      "id": "440261abfdf6821"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1540e44960e34f1"
      },
      "source": [
        "### GPT2\n",
        "\n",
        "GPT2是自回归式语言模型，可以根据前面的token预测下一个token。\n",
        "\n",
        "将上面的token id输入到GPT2模型中，就可以得到每个token的概率分布\n",
        "\n",
        "GPT2的输出的logits是一个三维张量，第一维是batch size，第二维是token的数量，第三维是词表的大小\n",
        "\n",
        "> 注意：GPT2输出的是logits，需要经过softmax才能得到真正的概率分布"
      ],
      "id": "c1540e44960e34f1"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61cc8893fe52ba45",
        "outputId": "574d965f-3b6b-4762-f442-d2fcb2322b30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 27, 50257])\n",
            "tensor([-31.8241, -31.4346, -33.4860,  ..., -39.5281, -38.9088, -31.8361])\n"
          ]
        }
      ],
      "source": [
        "input_ids = inputs.input_ids.to(device)\n",
        "with torch.no_grad():\n",
        "    logits = model(input_ids).logits\n",
        "print(logits.shape) # batch大小，序列长度，词表大小\n",
        "print(logits[0, 0, :]) # 对于第一个词的预测logits，通过softmax后可以得到概率分布"
      ],
      "id": "61cc8893fe52ba45"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9e1a150ae69ffef"
      },
      "source": [
        "## 计算Perplexity (PPL)\n",
        "\n",
        "PPL是语言模型的一个重要评价指标，表示模型对于给定的句子的概率分布的拟合程度。\n",
        "\n",
        "计算公式为：\n",
        "$$\n",
        "PPL = \\sqrt[N]{\\prod_{i=1}^{N}\\frac{1}{P(w_i|w_1,w_2,...,w_{i-1})}}\n",
        "$$\n",
        "通常可以转化为对数形式：\n",
        "$$\n",
        "PPL = \\exp\\left(\\frac{1}{N}\\sum_{i=1}^{N}-\\log P(w_i|w_1,w_2,...,w_{i-1})\\right)\n",
        "$$\n",
        "\n",
        "本节将实现GPT2模型的PPL计算"
      ],
      "id": "a9e1a150ae69ffef"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "f6b940c439179696"
      },
      "outputs": [],
      "source": [
        "from torch.nn import Softmax, CrossEntropyLoss\n",
        "\n",
        "\n",
        "def calculate_ppl(model, text):\n",
        "    ## TODO: 首先将文本转换为输入token (7分)\n",
        "    inputs = tokenizer(text,return_tensors=\"pt\")\n",
        "    input_ids = inputs.input_ids.to(device)\n",
        "\n",
        "    # 获取模型的输出\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids)\n",
        "        logits = outputs.logits\n",
        "        labels = input_ids.to(logits.device)\n",
        "        # GPT2每个位置都是预测下一个token的概率，所以需要将labels向左移动一位\n",
        "        shift_logits = logits[..., :-1, :]\n",
        "        shift_labels = labels[..., 1:]\n",
        "\n",
        "        ## TODO: 根据logits和labels计算model在text上的ppl（8分）\n",
        "        ## Hint: 可以直接通过Softmax获取概率值按照上面公式计算\n",
        "        ## Hint2: 也可以尝试利用CrossEntropyLoss进行等价计算\n",
        "\n",
        "        # 计算Cross Entropy Loss\n",
        "        loss_fct = CrossEntropyLoss()\n",
        "        # print(shift_logits)\n",
        "        # print(shift_labels)\n",
        "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "        # print(loss)\n",
        "\n",
        "        # 计算困惑度\n",
        "        ppl = torch.exp(loss)\n",
        "\n",
        "\n",
        "    return ppl"
      ],
      "id": "f6b940c439179696"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3992290369fa806d"
      },
      "source": [
        "## 测试"
      ],
      "id": "3992290369fa806d"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "638872fb9217c941",
        "outputId": "8cffac22-8cfa-4480-a215-bbb48037a979"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(68.2023)\n",
            "tensor(46.4538)\n"
          ]
        }
      ],
      "source": [
        "text1 = \"GPT-4 is a large multimodal model (accepting image and text inputs, emitting text outputs) that, while less capable than humans in many real-world scenarios, exhibits human-level performance on various professional and academic benchmarks.\"\n",
        "text2 = \"Until the rocket ship nearly imploded. On Nov. 17, OpenAI's nonprofit board of directors fired Altman, without warning or even much in the way of explanation. The surreal maneuvering that followed made the corporate dramas of Succession seem staid. Employees revolted. So did OpenAI's powerful investors; one even baselessly speculated that one of the directors who defenestrated Altman was a Chinese spy. The company's visionary chief scientist voted to oust his fellow co-founder, only to backtrack. Two interim CEOs came and went. The players postured via selfie, open letter, and heart emojis on social media. Meanwhile, the company's employees and its board of directors faced off in “a gigantic game of chicken,” says a person familiar with the discussions. At one point, OpenAI's whole staff threatened to quit if the board didn't resign and reinstall Altman within a few hours, three people involved in the standoff tell TIME. Then Altman looked set to decamp to Microsoft—with potentially hundreds of colleagues in tow. It seemed as if the company that catalyzed the AI boom might collapse overnight.\"\n",
        "\n",
        "print(calculate_ppl(model, text1))\n",
        "print(calculate_ppl(model, text2))"
      ],
      "id": "638872fb9217c941"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd741b855b3543a2"
      },
      "source": [
        "# 实验总结\n",
        "本次实验中，我们以GPT2模型为例，对PPL的计算进行了实践。我们采用先计算PPL的对数形式，再进行指数转换的方法来进行简化，利用了现有的 `CrossEntropyLoss()` 函数进行了计算。\n",
        "$$\n",
        "PPL = \\exp\\left(\\frac{1}{N}\\sum_{i=1}^{N}-\\log P(w_i|w_1,w_2,...,w_{i-1})=\\frac{1}{N}\\sum_{i=1}^{N}-1\\cdot \\log  P_i \\right)\n",
        "$$\n",
        "根据条件概率的定义，以及“GPT2每个位置都是预测下一个token的概率”的说明可知，shift_logits[i]在经过softmax和log之后就会得到位置 $i$ 上各个单词的概率，然后经过CrossEntropy，再和由label的one-hot编码在$w_i$(真实label)位置上那个唯一的1相乘得到 $-\\log P(w_i|w_1,w_2,...,w_{i-1})$。对shifts_logits进行操作，得到的便是\n",
        "$$\n",
        "\\left(\\frac{1}{N}\\sum_{i=1}^{N}-\\log P(w_i|w_1,w_2,...,w_{i-1})\\right)\n",
        "$$\n",
        "\n",
        "最后经过exp函数即可得到PPL分数。"
      ],
      "id": "dd741b855b3543a2"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}