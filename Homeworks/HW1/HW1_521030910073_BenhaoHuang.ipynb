{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 作业一：实现HMM中文分词和BPE英文分词\n",
    "姓名： 黄奔皓\n",
    "学号： 521030910073"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a5a31d058a6316b"
  },
  {
   "cell_type": "markdown",
   "id": "0a7a16ed",
   "metadata": {},
   "source": [
    "## 任务一：HMM模型用于中文分词"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030d9a94",
   "metadata": {},
   "source": [
    "任务一评分标准：\n",
    "1. 共有8处TODO需要填写，每个TODO计1-2分，共9分，预计代码量30行；\n",
    "2. 允许自行修改、编写代码完成，对于该情况，请补充注释以便于评分，否则结果不正确将导致较多的扣分；\n",
    "3. 用于说明实验的文字和总结不额外计分，但不写会导致扣分。\n",
    "\n",
    "注：本任务仅在短句子上进行效果测试，因此对概率的计算可直接进行连乘。在实践中，常先对概率取对数，将连乘变为加法来计算，以避免出现数值溢出的情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d77db9",
   "metadata": {},
   "source": [
    "导入HMM参数，初始化所需的起始概率矩阵，转移概率矩阵，发射概率矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b36e0db8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T08:12:16.422003Z",
     "start_time": "2023-11-04T08:12:16.373307Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d25beba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T08:12:16.425054Z",
     "start_time": "2023-11-04T08:12:16.377288Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"hmm_parameters.pkl\", \"rb\") as f:\n",
    "    hmm_parameters = pickle.load(f)\n",
    "\n",
    "# 非断字（B）为第0行，断字（I）为第1行\n",
    "# 发射概率矩阵中，词典大小为65536，以汉字的ord作为行key\n",
    "start_probability = hmm_parameters[\"start_prob\"]  # shape(2,)\n",
    "trans_matrix = hmm_parameters[\"trans_mat\"]  # shape(2, 2)\n",
    "emission_matrix = hmm_parameters[\"emission_mat\"]  # shape(2, 65536)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7070152",
   "metadata": {},
   "source": [
    "定义待处理的句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87219e30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T08:12:16.438734Z",
     "start_time": "2023-11-04T08:12:16.381888Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: 将input_sentence中的xxx替换为你的姓名（1分）\n",
    "input_sentence = \"黄奔皓是一名优秀的学生\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1035cbc7",
   "metadata": {},
   "source": [
    "实现viterbi算法，并以此进行中文分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1adac849",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T08:12:16.475213Z",
     "start_time": "2023-11-04T08:12:16.390293Z"
    }
   },
   "outputs": [],
   "source": [
    "def viterbi(sent_orig, start_prob, trans_mat, emission_mat):\n",
    "    \"\"\"\n",
    "    viterbi算法进行中文分词\n",
    "\n",
    "    Args:\n",
    "        sent_orig: str - 输入的句子\n",
    "        start_prob: numpy.ndarray - 起始概率矩阵\n",
    "        trans_mat: numpy.ndarray - 转移概率矩阵\n",
    "        emission_mat: numpy.ndarray - 发射概率矩阵\n",
    "\n",
    "    Return:\n",
    "        str - 中文分词的结果\n",
    "    \"\"\"\n",
    "\n",
    "    #  将汉字转为数字表示\n",
    "    sent_ord = [ord(x) for x in sent_orig]\n",
    "\n",
    "    # `dp`用来储存不同位置每种标注（B/I）的最大概率值\n",
    "    dp = np.zeros((2, len(sent_ord)), dtype=float)\n",
    "\n",
    "    # `path`用来储存最大概率对应的上步B/I选择\n",
    "    #  例如 path[1][7] == 1 意味着第8个（从1开始计数）字符标注I对应的最大概率，其前一步的隐状态为1（I）\n",
    "    #  例如 path[0][5] == 1 意味着第6个字符标注B对应的最大概率，其前一步的隐状态为1（I）\n",
    "    #  例如 path[1][1] == 0 意味着第2个字符标注I对应的最大概率，其前一步的隐状态为0（B）\n",
    "    path = np.zeros((2, len(sent_ord)), dtype=int)\n",
    "\n",
    "    #  TODO: 第一个位置的最大概率值计算（1分）\n",
    "    dp[0][0] = start_prob[0] * emission_mat[0][sent_ord[0]]\n",
    "    dp[1][0] = start_prob[1] * emission_mat[1][sent_ord[1]]\n",
    "    signs = [0,1]\n",
    "    #  TODO: 其余位置的最大概率值计算（填充dp和path矩阵）（2分）\n",
    "    for i in range(1, len(sent_ord)): # 词\n",
    "        for label in signs: # 列\n",
    "            value = []\n",
    "            for pre_label in signs: # 上一列，找最大值\n",
    "                value.append(dp[pre_label][i - 1] * trans_mat[pre_label][label] * emission_mat[label][sent_ord[i]])\n",
    "            max_id = np.argmax(value)\n",
    "            path[label][i] = max_id\n",
    "            dp[label][i] = value[max_id]\n",
    "        \n",
    "\n",
    "    #  `labels`用来储存每个位置最有可能的隐状态\n",
    "    labels = [0 for _ in range(len(sent_ord))]\n",
    "\n",
    "    #  TODO: 计算labels每个位置上的值（填充labels矩阵）（1分）\n",
    "    for p in range(len(labels)-1,0,-1):\n",
    "        if p == len(labels) - 1:\n",
    "            max_label = np.argmax(dp[:,p]) # 最后一项的label由概率直接选\n",
    "            labels[p] = max_label\n",
    "        else:\n",
    "            previous_label = path[labels[p + 1]][p + 1]\n",
    "            labels[p] = previous_label\n",
    "\n",
    "    #  根据lalels生成切分好的字符串\n",
    "    sent_split = []\n",
    "    for idx, label in enumerate(labels):\n",
    "        if label == 1:\n",
    "            sent_split += [sent_ord[idx], ord(\"/\")]\n",
    "        else:\n",
    "            sent_split += [sent_ord[idx]]\n",
    "    sent_split_str = \"\".join([chr(x) for x in sent_split])\n",
    "\n",
    "    return sent_split_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d795414b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T08:12:16.476956Z",
     "start_time": "2023-11-04T08:12:16.394506Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "viterbi算法分词结果： 黄奔/皓是/一名/优秀/的/学生/\n"
     ]
    }
   ],
   "source": [
    "print(\"viterbi算法分词结果：\", viterbi(input_sentence, start_probability, trans_matrix, emission_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fcafdb",
   "metadata": {},
   "source": [
    "实现前向算法，计算该句子的概率值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf6796a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T08:12:16.500610Z",
     "start_time": "2023-11-04T08:12:16.400189Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_prob_by_forward(sent_orig, start_prob, trans_mat, emission_mat):\n",
    "    \"\"\"\n",
    "    前向算法，计算输入中文句子的概率值\n",
    "\n",
    "    Args:\n",
    "        sent_orig: str - 输入的句子\n",
    "        start_prob: numpy.ndarray - 起始概率矩阵\n",
    "        trans_mat: numpy.ndarray - 转移概率矩阵\n",
    "        emission_mat: numpy.ndarray - 发射概率矩阵\n",
    "\n",
    "    Return:\n",
    "        float - 概率值\n",
    "    \"\"\"\n",
    "\n",
    "    #  将汉字转为数字表示\n",
    "    sent_ord = [ord(x) for x in sent_orig]\n",
    "\n",
    "    # `dp`用来储存不同位置每种隐状态（B/I）下，到该位置为止的句子的概率\n",
    "    dp = np.zeros((2, len(sent_ord)), dtype=float)\n",
    "\n",
    "    # TODO: 初始位置概率的计算（1分）\n",
    "    dp[0][0] = start_prob[0] * emission_mat[0][sent_ord[0]]\n",
    "    dp[1][0] = start_prob[1] * emission_mat[1][sent_ord[0]]\n",
    "\n",
    "    # TODO: 先计算其余位置的概率（填充dp矩阵），然后return概率值（1分）\n",
    "    signs = [0, 1]\n",
    "    for i in range(1, len(sent_ord)): # 词\n",
    "        for label in signs: # 列\n",
    "            value = 0\n",
    "            for pre_label in signs: # 上一列，找最大值\n",
    "                value += (dp[pre_label][i - 1] * trans_mat[pre_label][label] * emission_mat[label][sent_ord[i]])\n",
    "            dp[label][i] = value\n",
    "    return sum([dp[i][len(sent_ord) - 1] for i in range(2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59533cd8",
   "metadata": {},
   "source": [
    "实现后向算法，计算该句子的概率值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e898306",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T08:12:16.502447Z",
     "start_time": "2023-11-04T08:12:16.405693Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_prob_by_backward(sent_orig, start_prob, trans_mat, emission_mat):\n",
    "    \"\"\"\n",
    "    后向算法，计算输入中文句子的概率值\n",
    "\n",
    "    Args:\n",
    "        sent_orig: str - 输入的句子\n",
    "        start_prob: numpy.ndarray - 起始概率矩阵\n",
    "        trans_mat: numpy.ndarray - 转移概率矩阵\n",
    "        emission_mat: numpy.ndarray - 发射概率矩阵\n",
    "\n",
    "    Return:\n",
    "        float - 概率值\n",
    "    \"\"\"\n",
    "\n",
    "    #  将汉字转为数字表示\n",
    "    sent_ord = [ord(x) for x in sent_orig]\n",
    "\n",
    "    # `dp`用来储存不同位置每种隐状态（B/I）下，从结尾到该位置为止的句子的概率\n",
    "    dp = np.zeros((2, len(sent_ord)), dtype=float)\n",
    "\n",
    "    # TODO: 终末位置概率的初始化（1分）\n",
    "    dp[0][len(sent_ord) - 1] = 1\n",
    "    dp[1][len(sent_ord) - 1] = 1\n",
    "\n",
    "    # TODO: 先计算其余位置的概率（填充dp矩阵），然后return概率值（1分）\n",
    "    signs = [0, 1]\n",
    "    for i in range(len(sent_ord) - 1, 0, -1): # 词\n",
    "        for label in signs: # 列\n",
    "            value = 0\n",
    "            for pre_label in signs: # 上一列，找最大值\n",
    "                value += (dp[pre_label][i] * trans_mat[label][pre_label] * emission_mat[pre_label][sent_ord[i]])\n",
    "            dp[label][i - 1] = value\n",
    "\n",
    "    return sum([dp[i][0] * start_prob[i] * emission_mat[i][sent_ord[0]] for i in range(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b26101d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T08:12:16.507554Z",
     "start_time": "2023-11-04T08:12:16.409356Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前向算法概率： 8.380526777122497e-36\n",
      "后向算法概率： 8.380526777122501e-36\n"
     ]
    }
   ],
   "source": [
    "print(\"前向算法概率：\", compute_prob_by_forward(input_sentence, start_probability, trans_matrix, emission_matrix))\n",
    "print(\"后向算法概率：\", compute_prob_by_backward(input_sentence, start_probability, trans_matrix, emission_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 实验总结\n",
    "\n",
    "在此次试验中，我实现了HMM的viterbi算法，用于中文分词；并实现了前向算法和后向算法，计算了某句子生成的概率值，复习回顾了林老师教授的知识，能做到掌握和应用。实验过程中，我的难点在于理解动态规划和ppt上转移方程的对应关系，这需要耐心细致。\n",
    "\n",
    "## viterbi算法流程\n",
    "1. 初始化句子第一个词不同label的概率\n",
    "2. 对下一个词的所有可能标签(CD为例)，根据 $\\max _{l e f t}(\\text { left } \\cdot P(\\mathrm{CD} \\mid \\text { left_tag }) P(\\mathrm{Jobs} \\mid \\mathrm{CD}))$ 进行计算。\n",
    "3. 重复步骤2，直到最后一个单词\n",
    "4. 在最后一个单词各可能label的概率选出最大的一个label，进而进行回溯。\n",
    "\n",
    "## 前向算法\n",
    "前向算法和viterbi算法的流程比，省去了回溯的过程，且状态转移方程变为求某个单词的所有可能label的数值和。掌握viterbi算法后，前向算法并不难实现。\n",
    "\n",
    "## 后向算法\n",
    "后向算法从句子末端的单词开始进行计算，最后与前向算法所得结果数值上基本一致。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20803f6a1a465dd6"
  },
  {
   "cell_type": "markdown",
   "id": "e994be6e",
   "metadata": {},
   "source": [
    "## 任务二：BPE算法用于英文分词"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cc4775",
   "metadata": {},
   "source": [
    "任务二评分标准：\n",
    "\n",
    "1. 共有7处TODO需要填写，每个TODO计1-2分，共9分，预计代码量50行；\n",
    "2. 允许自行修改、编写代码完成，对于该情况，请补充注释以便于评分，否则结果不正确将导致较多的扣分；\n",
    "3. 用于说明实验的文字和总结不额外计分，但不写会导致扣分。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb5dbb9",
   "metadata": {},
   "source": [
    "构建空格分词器，将语料中的句子以空格切分成单词，然后将单词拆分成字母加`</w>`的形式。例如`apple`将变为`a p p l e </w>`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70e10703",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T08:12:16.511677Z",
     "start_time": "2023-11-04T08:12:16.413853Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6c3667a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T08:12:16.511872Z",
     "start_time": "2023-11-04T08:12:16.417703Z"
    }
   },
   "outputs": [],
   "source": [
    "_splitor_pattern = re.compile(r\"[^a-zA-Z']+|(?=')\")\n",
    "_digit_pattern = re.compile(r\"\\d+\")\n",
    "\n",
    "\n",
    "def white_space_tokenize(corpus):\n",
    "    \"\"\"\n",
    "    先正则化（字母转小写、数字转为N、除去标点符号），然后以空格分词语料中的句子，例如：\n",
    "    输入 corpus=[\"I am happy.\", \"I have 10 apples!\"]，\n",
    "    得到 [[\"i\", \"am\", \"happy\"], [\"i\", \"have\", \"N\", \"apples\"]]\n",
    "\n",
    "    Args:\n",
    "        corpus: List[str] - 待处理的语料\n",
    "\n",
    "    Return:\n",
    "        List[List[str]] - 二维List，内部的List由每个句子的单词str构成\n",
    "    \"\"\"\n",
    "\n",
    "    tokeneds = [list(filter(lambda tkn: len(tkn) > 0, _splitor_pattern.split(_digit_pattern.sub(\"N\", stc.lower())))) for stc in corpus]\n",
    "\n",
    "    return tokeneds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3732502a",
   "metadata": {},
   "source": [
    "编写相应函数构建BPE算法需要用到的初始状态词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7bf823e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T08:12:16.513022Z",
     "start_time": "2023-11-04T08:12:16.421756Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_bpe_vocab(corpus):\n",
    "    \"\"\"\n",
    "    将语料进行white_space_tokenize处理后，将单词每个字母以空格隔开、结尾加上</w>后，构建带频数的字典，例如：\n",
    "    输入 corpus=[\"I am happy.\", \"I have 10 apples!\"]，\n",
    "    得到\n",
    "    {\n",
    "        'i </w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'h a p p y </w>': 1,\n",
    "        'h a v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a p p l e s </w>': 1\n",
    "     }\n",
    "\n",
    "    Args:\n",
    "        corpus: List[str] - 待处理的语料\n",
    "\n",
    "    Return:\n",
    "        Dict[str, int] - \"单词分词状态->频数\"的词典\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_corpus = white_space_tokenize(corpus)\n",
    "\n",
    "    bpe_vocab = dict()\n",
    "\n",
    "    # TODO: 完成函数体（1分）\n",
    "    for sentence in tokenized_corpus:\n",
    "        for item in sentence:\n",
    "            item = \" \".join(item) + ' </w>'\n",
    "            if item in bpe_vocab:\n",
    "                bpe_vocab[item] += 1\n",
    "            else:\n",
    "                bpe_vocab[item] = 1\n",
    "\n",
    "    return bpe_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d25245",
   "metadata": {},
   "source": [
    "编写所需的其他函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "087d11e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T08:12:16.521501Z",
     "start_time": "2023-11-04T08:12:16.426570Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_bigram_freq(bpe_vocab):\n",
    "    \"\"\"\n",
    "    统计\"单词分词状态->频数\"的词典中，各bigram的频次（假设该词典中，各个unigram以空格间隔），例如：\n",
    "    输入 bpe_vocab=\n",
    "    {\n",
    "        'i </w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'h a p p y </w>': 1,\n",
    "        'h a v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a p p l e s </w>': 1\n",
    "    }\n",
    "    得到\n",
    "    {\n",
    "        ('i', '</w>'): 2,\n",
    "        ('a', 'm'): 1,\n",
    "        ('m', '</w>'): 1,\n",
    "        ('h', 'a'): 2,\n",
    "        ('a', 'p'): 2,\n",
    "        ('p', 'p'): 2,\n",
    "        ('p', 'y'): 1,\n",
    "        ('y', '</w>'): 1,\n",
    "        ('a', 'v'): 1,\n",
    "        ('v', 'e'): 1,\n",
    "        ('e', '</w>'): 1,\n",
    "        ('N', '</w>'): 1,\n",
    "        ('p', 'l'): 1,\n",
    "        ('l', 'e'): 1,\n",
    "        ('e', 's'): 1,\n",
    "        ('s', '</w>'): 1\n",
    "    }\n",
    "\n",
    "    Args:\n",
    "        bpe_vocab: Dict[str, int] - \"单词分词状态->频数\"的词典\n",
    "\n",
    "    Return:\n",
    "        Dict[Tuple(str, str), int] - \"bigram->频数\"的词典\n",
    "    \"\"\"\n",
    "\n",
    "    bigram_freq = dict()\n",
    "\n",
    "    # TODO: 完成函数体（1分）\n",
    "    for key in bpe_vocab.keys():\n",
    "        lst = key.split(' ')\n",
    "        for i in range(len(lst) - 1):\n",
    "            t = (lst[i],lst[i + 1])\n",
    "            if t in bigram_freq.keys():\n",
    "                bigram_freq[t] += bpe_vocab[key]\n",
    "            else:\n",
    "                bigram_freq[t] = bpe_vocab[key]\n",
    "\n",
    "    return bigram_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ba426043",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T08:12:16.521704Z",
     "start_time": "2023-11-04T08:12:16.431844Z"
    }
   },
   "outputs": [],
   "source": [
    "def refresh_bpe_vocab_by_merging_bigram(bigram, old_bpe_vocab):\n",
    "    \"\"\"\n",
    "    在\"单词分词状态->频数\"的词典中，合并指定的bigram（即去掉对应的相邻unigram之间的空格），最后返回新的词典，例如：\n",
    "    输入 bigram=('i', '</w>')，old_bpe_vocab=\n",
    "    {\n",
    "        'i </w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'h a p p y </w>': 1,\n",
    "        'h a v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a p p l e s </w>': 1\n",
    "    }\n",
    "    得到\n",
    "    {\n",
    "        'i</w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'h a p p y </w>': 1,\n",
    "        'h a v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a p p l e s </w>': 1\n",
    "    }\n",
    "\n",
    "    Args:\n",
    "        old_bpe_vocab: Dict[str, int] - 初始\"单词分词状态->频数\"的词典\n",
    "\n",
    "    Return:\n",
    "        Dict[str, int] - 合并后的\"单词分词状态->频数\"的词典\n",
    "    \"\"\"    \n",
    "\n",
    "    # TODO: 完成函数体（1分）\n",
    "    new_bpe_vocab = dict()\n",
    "    for key in old_bpe_vocab.keys():\n",
    "        lst = key.split(' ')\n",
    "        record = []\n",
    "        for i in range(len(lst) - 1):\n",
    "            if lst[i] == bigram[0]:\n",
    "                if lst[i + 1] == bigram[1]:\n",
    "                    record.append(i)\n",
    "        new_key = ''\n",
    "        for k in range(len(lst)):\n",
    "            if k in record:\n",
    "                new_key += lst[k]\n",
    "            else:\n",
    "                if k == len(lst) - 1:\n",
    "                    new_key += (lst[k])\n",
    "                else:\n",
    "                    new_key += (lst[k] + ' ')\n",
    "        new_bpe_vocab[new_key] = old_bpe_vocab[key]\n",
    "\n",
    "    return new_bpe_vocab\n",
    "\n",
    "print(refresh_bpe_vocab_by_merging_bigram(bigram=('i', 'b'), old_bpe_vocab=\n",
    "    {\n",
    "        'i ba </w>': 2, # 出现ba是有可能的，b,a bi-gram合并但是单个b也可以保留，这个例子如果直接替换不太对\n",
    "        'a m i </w>': 1,\n",
    "        'h a p p y </w>': 1,\n",
    "        'h a v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a p p l e s </w>': 1\n",
    "    }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "992438a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T08:12:16.522425Z",
     "start_time": "2023-11-04T08:12:16.437759Z"
    }
   },
   "outputs": [],
   "source": [
    "def sort_by_len(token):\n",
    "    '''\n",
    "    used for sort key\n",
    "    '''\n",
    "    if '</w>' in token[0]:\n",
    "        return len(token[0]) - 3 # '</w>' should be treated as length 1\n",
    "    else:\n",
    "        return len(token[0])\n",
    "\n",
    "def get_bpe_tokens(bpe_vocab):\n",
    "    \"\"\"\n",
    "    根据\"单词分词状态->频数\"的词典，返回所得到的BPE分词列表，并将该列表按照分词长度降序排序返回，例如：\n",
    "    输入 bpe_vocab=\n",
    "    {\n",
    "        'i</w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'ha pp y </w>': 1,\n",
    "        'ha v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a pp l e s </w>': 1\n",
    "    }\n",
    "    得到\n",
    "    [\n",
    "        ('i</w>', 2),\n",
    "        ('ha', 2),\n",
    "        ('pp', 2),\n",
    "        ('a', 2),\n",
    "        ('m', 1),\n",
    "        ('</w>', 5),\n",
    "        ('y', 1),\n",
    "        ('v', 1),\n",
    "        ('e', 2),\n",
    "        ('N', 1),\n",
    "        ('l', 1),\n",
    "        ('s', 1)\n",
    "     ]\n",
    "\n",
    "    Args:\n",
    "        bpe_vocab: Dict[str, int] - \"单词分词状态->频数\"的词典\n",
    "\n",
    "    Return:\n",
    "        List[Tuple(str, int)] - BPE分词和对应频数组成的List\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: 完成函数体（2分）\n",
    "    tmp_dict = dict()\n",
    "    for key in bpe_vocab.keys():\n",
    "        lst = key.split(' ')\n",
    "        for token in lst:\n",
    "            if token in tmp_dict.keys():\n",
    "                tmp_dict[token] += bpe_vocab[key]\n",
    "            else:\n",
    "                tmp_dict[token] = bpe_vocab[key]\n",
    "    bpe_tokens = []\n",
    "    for key in tmp_dict.keys():\n",
    "        bpe_tokens.append((key,tmp_dict[key]))\n",
    "    bpe_tokens.sort(key=sort_by_len,reverse=True)\n",
    "    \n",
    "    \n",
    "    return bpe_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c56995e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T08:12:16.522518Z",
     "start_time": "2023-11-04T08:12:16.442986Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_bpe_tokenize(word, bpe_tokens):\n",
    "    \"\"\"\n",
    "    根据按长度降序的BPE分词列表，将所给单词进行BPE分词，最后打印结果。\n",
    "    \n",
    "    思想是，对于一个待BPE分词的单词，按照长度顺序从列表中寻找BPE分词进行子串匹配，\n",
    "    若成功匹配，则对该子串左右的剩余部分递归地进行下一轮匹配，直到剩余部分长度为0，\n",
    "    或者剩余部分无法匹配（该部分整体由\"<unknown>\"代替）\n",
    "    \n",
    "    例1：\n",
    "    输入 word=\"supermarket\", bpe_tokens=[\n",
    "        (\"su\", 20),\n",
    "        (\"are\", 10),\n",
    "        (\"per\", 30),\n",
    "    ]\n",
    "    最终打印 \"su per <unknown>\"\n",
    "\n",
    "    例2：\n",
    "    输入 word=\"shanghai\", bpe_tokens=[\n",
    "        (\"hai\", 1),\n",
    "        (\"sh\", 1),\n",
    "        (\"an\", 1),\n",
    "        (\"</w>\", 1),\n",
    "        (\"g\", 1)\n",
    "    ]\n",
    "    最终打印 \"sh an g hai </w>\"\n",
    "\n",
    "    Args:\n",
    "        word: str - 待分词的单词str\n",
    "        bpe_tokens: List[Tuple(str, int)] - BPE分词和对应频数组成的List\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: 请尝试使用递归函数定义该分词过程（2分）\n",
    "    def bpe_tokenize(sub_word):\n",
    "        if len(sub_word) == 0:\n",
    "            return ''\n",
    "        for token in bpe_tokens:\n",
    "            token = token[0]\n",
    "            i = sub_word.find(token)\n",
    "            if i == -1:\n",
    "                continue\n",
    "            else:\n",
    "                left = sub_word[:i]\n",
    "                right = sub_word[i + len(token):]\n",
    "                left_res = bpe_tokenize(left)\n",
    "                right_res = bpe_tokenize(right)\n",
    "                if left_res != '':\n",
    "                    left_res = left_res + ' '\n",
    "                if right_res != '':\n",
    "                    right_res = ' ' + right_res \n",
    "                res = left_res + token  + right_res\n",
    "                return res\n",
    "        return \"<unknown>\"\n",
    "\n",
    "    res = bpe_tokenize(word + \"</w>\")\n",
    "    print(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd70402",
   "metadata": {},
   "source": [
    "开始读取数据集并训练BPE分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "215b56d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T08:12:16.754441Z",
     "start_time": "2023-11-04T08:12:16.445973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training corpus.\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/news.2007.en.shuffled.deduped.train\", encoding=\"utf-8\") as f:\n",
    "    training_corpus = list(map(lambda l: l.strip(), f.readlines()[:1000]))\n",
    "\n",
    "print(\"Loaded training corpus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7bccd41d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T08:12:22.728060Z",
     "start_time": "2023-11-04T08:12:16.773287Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:05<00:00, 50.49it/s]\n"
     ]
    }
   ],
   "source": [
    "def sort_by_freq(item):\n",
    "    '''\n",
    "    sort bi-gram freq\n",
    "    '''\n",
    "    return item[1]\n",
    "\n",
    "from tqdm import tqdm\n",
    "training_iter_num = 300\n",
    "training_bpe_vocab = build_bpe_vocab(training_corpus)\n",
    "for i in tqdm(range(training_iter_num),total=training_iter_num):\n",
    "    # TODO: 完成训练循环内的代码逻辑（2分）\n",
    "    bigram_freq = get_bigram_freq(training_bpe_vocab)\n",
    "\n",
    "    bigram_freq_items = list(bigram_freq.items())\n",
    "    bigram_freq_items.sort(key = sort_by_freq,reverse = True)\n",
    "    most_freq_item = bigram_freq_items[0]\n",
    "    training_bpe_vocab = refresh_bpe_vocab_by_merging_bigram(bigram=most_freq_item[0],old_bpe_vocab=training_bpe_vocab)\n",
    "training_bpe_tokens = get_bpe_tokens(training_bpe_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ea3ddd",
   "metadata": {},
   "source": [
    "测试BPE分词器的分词效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c0cfdb29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T08:12:22.729592Z",
     "start_time": "2023-11-04T08:12:22.725523Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naturallanguageprocessing 的分词结果为：\n"
     ]
    },
    {
     "data": {
      "text/plain": "'n a tur all an g u ag e pro c es s ing</w>'"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_word = \"naturallanguageprocessing\"\n",
    "\n",
    "print(\"naturallanguageprocessing 的分词结果为：\")\n",
    "print_bpe_tokenize(test_word, training_bpe_tokens)\n",
    "for item in training_bpe_tokens:\n",
    "    if item[0] == 'w':\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:09<00:00, 54.38it/s]\n",
      "100%|██████████| 1000/1000 [00:16<00:00, 60.81it/s]\n",
      "100%|██████████| 1500/1500 [00:22<00:00, 65.58it/s]\n",
      "100%|██████████| 2000/2000 [00:29<00:00, 68.18it/s]\n",
      "100%|██████████| 2500/2500 [00:35<00:00, 70.20it/s]\n",
      " 23%|██▎       | 687/3000 [00:17<00:35, 64.86it/s]"
     ]
    }
   ],
   "source": [
    "training_iter_nums = [i for i in range(500,10000,500)]\n",
    "res = []\n",
    "for training_iter_num in training_iter_nums:\n",
    "    training_bpe_vocab = build_bpe_vocab(training_corpus)\n",
    "    for i in tqdm(range(training_iter_num),total=training_iter_num):\n",
    "        # TODO: 完成训练循环内的代码逻辑（2分）\n",
    "        bigram_freq = get_bigram_freq(training_bpe_vocab)\n",
    "    \n",
    "        bigram_freq_items = list(bigram_freq.items())\n",
    "        bigram_freq_items.sort(key = sort_by_freq,reverse = True)\n",
    "        most_freq_item = bigram_freq_items[0]\n",
    "        training_bpe_vocab = refresh_bpe_vocab_by_merging_bigram(bigram=most_freq_item[0],old_bpe_vocab=training_bpe_vocab)\n",
    "    training_bpe_tokens = get_bpe_tokens(training_bpe_vocab)\n",
    "    res.append(print_bpe_tokenize(test_word, training_bpe_tokens))\n",
    "np.save(\"res.npy\",res)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-11-04T08:12:22.728817Z"
    }
   },
   "id": "93208ee99e017b4a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 实验总结\n",
    "在该实验中，我初步完成了BPE分词。其算法流程如下：\n",
    "1. 初始词汇表：将语料中的单词和其频数提取出来\n",
    "2. 迭代：\n",
    "（1）从词汇表中计算排序得到频数最高的bi-gram，将之捏合为新的token\n",
    "（2）将原词汇表中的这个bi-gram替换为新的token，得到新的词汇表\n",
    "\n",
    "3. 终止：迭代达到预先设定的最大次数时停止\n",
    "\n",
    "经过实验发现，分词的效果和迭代次数有很大关系. 迭代次数并非越多越好：\n",
    "* 300iter: n a tur all an g u ag e pro c es s ing</w>，迭代次数仍不足，分词中仍出现大量单字母。\n",
    "* 5000iter: n a tur al langu ag e pro c es sing</w>，迭代次数适中，分词结果较好。\n",
    "* 10000iter: <unknown> t ur <unknown> ang <unknown> e <unknown> o c es s ing</w>，迭代次数过多，合并的bi-gram过多，导致出现 <unknown>\n",
    "\n",
    "因此，想要获得好的效果，用一个合适的training iteration num来训练BPE分词器非常重要。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "977b6d7250854946"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-11-04T08:14:35.564651Z"
    }
   },
   "id": "6f32d448446152cd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
